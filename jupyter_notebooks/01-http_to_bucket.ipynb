{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3df9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlt[s3] pyarrow python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89debc2e-b7ec-40ad-98de-bd9e02b30751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyarrow.parquet as pq\n",
    "import fsspec # Filesystem specification for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36903e20",
   "metadata": {},
   "source": [
    "In the following code we're using fsspec since it gives you a unified layer for file access, regardless of whether the files are on your local disk, in MinIO, in Azure, in S3, or at a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70dd7a8-28d7-46d6-9e1a-9dc90d884aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a DLT resource to load data from a Parquet file at a given URL\n",
    "@dlt.resource(table_name=\"df_data\")\n",
    "def my_df():\n",
    "    parquet_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\"\n",
    "    with fsspec.open(parquet_url, mode=\"rb\") as f:\n",
    "        table = pq.read_table(f)\n",
    "        yield table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4289265",
   "metadata": {},
   "source": [
    "Before running the following code make sure to have your credentials in `.dlt/secrets.toml` You can use `secrets-template.toml` as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ca16023-de77-4df7-8829-fea333da1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸš€ Pipeline configuration\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"parquet_to_minio\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"taxis_parquet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9fc2b2-b417-4aa8-ae1d-e4ef138365cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline parquet_to_minio load step completed in 2.55 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset taxis_parquet\n",
      "The filesystem destination used s3://taxis location to store data\n",
      "Load package 1757342683.0509217 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "load_info = pipeline.run(\n",
    "    my_df,\n",
    "    loader_file_format=\"parquet\",\n",
    "    write_disposition=\"replace\"\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4f484e",
   "metadata": {},
   "source": [
    "> In case of having errors while setting up the secrets.toml for DLT keep in mind you can also do this by setting the secrets as enviroment variables. You just need to keep in mind the dlt naming standard. The following code will create an .env file as reference and you only need to add the values of your secrets once the file is created. \n",
    "\n",
    "Here's the [docs](https://dlthub.com/docs/general-usage/credentials/setup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb30f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# DO NOT SET YOUR VALUES HERE, THIS IS JUST A TEMPLATE\n",
    "# YOU MUST SETUP YOUR CREDENTIALS ONCE THE FILE IS CREATED\n",
    "env_path = os.path.join(os.getcwd(), \"../.env\")\n",
    "env_content = \"\"\"# ============================\n",
    "# Environment variables for DLT\n",
    "# Fill in your credentials below\n",
    "# ============================\n",
    "\n",
    "# -- Destination (Azure ADLS) --\n",
    "DESTINATION__FILESYSTEM__CREDENTIALS__AZURE_STORAGE_ACCOUNT_NAME=your_account_name\n",
    "DESTINATION__FILESYSTEM__CREDENTIALS__AZURE_STORAGE_ACCOUNT_KEY=your_account_key\n",
    "DESTINATION__FILESYSTEM__BUCKET_URL=your_bucket_url\n",
    "\n",
    "# -- Source (S3 / MinIO) --\n",
    "SOURCES__FILESYSTEM__CREDENTIALS__AWS_ACCESS_KEY_ID=randomuser\n",
    "SOURCES__FILESYSTEM__CREDENTIALS__AWS_SECRET_ACCESS_KEY=randompassword\n",
    "SOURCES__FILESYSTEM__CREDENTIALS__ENDPOINT_URL=http://minio:9000 # Change if needed\n",
    "\n",
    "# -- Bucket / File URLs --\n",
    "DESTINATION__FILESYSTEM__BUCKET_URL=\n",
    "SOURCES__FILESYSTEM__BUCKET_URL=\n",
    "\"\"\"\n",
    "\n",
    "# Write the .env file\n",
    "with open(env_path, \"w\") as f:\n",
    "    f.write(env_content)\n",
    "\n",
    "print(f\".env file created at: {env_path}\")\n",
    "print(\"ðŸ‘‰ Please edit this file and replace placeholders with your actual credentials.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1382923",
   "metadata": {},
   "source": [
    "Once you have edited your credentials run the following code to load them. and then run again the `pipeline.run`\n",
    "\n",
    "> If you're using this approach make sure to erase the secrets.toml files and restart your enviroment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904a2c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "\n",
    "env_path = os.path.abspath(\"../.env\")\n",
    "print(\"Looking for .env at:\", env_path)\n",
    "\n",
    "# Make sure the file exists\n",
    "print(\"File exists?\", os.path.isfile(env_path))\n",
    "\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Check if the variables are loaded\n",
    "print(\"Azure account:\", os.getenv(\"DESTINATION__FILESYSTEM__CREDENTIALS__AZURE_STORAGE_ACCOUNT_NAME\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lakehouse-oss-vs-databricks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
