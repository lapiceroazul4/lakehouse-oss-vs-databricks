{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd0f536",
   "metadata": {},
   "source": [
    "# **Pipeline** - **Parquet** *(from HTTP endpoint)* **to MinIO Bucket**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7533218",
   "metadata": {},
   "source": [
    "## **Librerías**\n",
    "\n",
    "- **`dlt`**\n",
    " → Framework para definir pipelines de extracción, transformación y carga *(ETL/ELT)*.\n",
    "\n",
    "- **`pyarrow.parquet`** (`pq`)\n",
    "→ Módulo de Apache Arrow usado para leer el archivo Parquet como tabla Arrow.\n",
    "\n",
    "- **`fsspec`**\n",
    "→ Esta librería unifica el acceso a sistemas de archivos con una sola API, abstrae backends como:\n",
    "    - **`s3fs`** *(para AWS S3 y compatibles como MinIO)*,\n",
    "    - **`adlfs`** *(para Azure Data Lake / Blob Storage)*,\n",
    "    - **`gcsfs`** *(para Google Cloud Storage)*,\n",
    "    - y también acceso a local filesystem, HTTP/FTP, HDFS, entre otros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89debc2e-b7ec-40ad-98de-bd9e02b30751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import pyarrow.parquet as pq\n",
    "import fsspec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf52457",
   "metadata": {},
   "source": [
    "## **Extrayendo el Parquet del endpoint HTTP**\n",
    "\n",
    "Para extraer el archivo se empieza definiendo un recurso de dlt (`@dlt.resource`) mediante [un decorador propio de la librería](https://dlthub.com/docs/general-usage/resource): este recurso permite construir un flujo de datos a partir de los datos que le entreguemos.\n",
    "\n",
    "* dlt permite flujos de datos con una [carga **incremental**](https://dlthub.com/docs/general-usage/incremental-loading). Su aplicación en dlt se observa desde cuando hacemos yield en una función para retornar los datos iterativamente.\n",
    "\n",
    "    * `yield` convierte a las funciones en un funciones generadoras: resulta necesario iterar *(mediante la palabra clave `next()`, aunque esto es lo que posiblemente se hace detrás)*.\n",
    "\n",
    "    * En este caso no se aplica en su máximo esplendor *→ Esta carga de datos es un **Full load***.\n",
    "\n",
    "* [ `fsspec.open()`](https://filesystem-spec.readthedocs.io/en/latest/api.html#fsspec.open) es usado aquí para poder extraer el Parquet del endpoint. Este archivo es leído como binario (`mode=\"rb\"`), dada la naturaleza de este formato.\n",
    "\n",
    "* Para transformar estos binarios en un formato que sea eficiente (y que no mate al computador) usamos la librería PyArrow con su módulo pyarrow.parquet para que este pueda leer archivos Parquet y, posteriormente, leer este mismo como una tabla Arrow.\n",
    "\n",
    "    *  Estas tablas [son parecidas a los dataframes de Pandas](https://medium.com/data-engineering-with-dremio/getting-started-with-data-analytics-using-pyarrow-in-python-ac7a100bc569), pero están diseñadas específicamente para soportar el formato columnar que maneja Apache Arrow.\n",
    "\n",
    "    * Pueden ser particionadas y procesadas en paralelo para mejor el rendimiento en datasets más grandes.\n",
    "\n",
    "* Si bien desde aquí dlt ya podría realizar la carga como Parquet al bucket, es valioso añadir que en caso de que quisiéramos transformar este formato a una dataframe de Pandas basta solo con usar el método `to_pandas()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70dd7a8-28d7-46d6-9e1a-9dc90d884aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define un recurso que lee datos de un parquet remoto\n",
    "@dlt.resource(table_name=\"df_data\")\n",
    "def my_df():\n",
    "    parquet_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2025-01.parquet\"\n",
    "    with fsspec.open(parquet_url, mode=\"rb\") as f:\n",
    "        table = pq.read_table(f)\n",
    "        yield table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4791c1",
   "metadata": {},
   "source": [
    "## **Instanciando el pipeline**\n",
    "Instanciar un pipeline nos permite mover los datos que extraemos a un destino. Este método acepta tanto [*sources*](https://dlthub.com/docs/general-usage/source) como [*resources*](https://dlthub.com/docs/general-usage/resource) de dlt.\n",
    "\n",
    "- Para mandarlo a un bucket *(ya sea de Azure BS, Amazon S3 o MinIO)* usamos siempre  `destination=\"filesystem\"`.\n",
    "\n",
    "- `dataset_name` indica el *namespace* que se va a usar para los datos que se van a guardar.\n",
    "\n",
    "- No es necesario declarar ninguna credencial dentro de este método ni en ningún otro que vayamos a usar; ya dlt las lee desde el archivo `secrets.toml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca16023-de77-4df7-8829-fea333da1e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"parquet_to_minio\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"taxis_parquet\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090092f",
   "metadata": {},
   "source": [
    "## **Ejecutando el pipeline**\n",
    "\n",
    "Con este método podemos ejecutar el resource que declaramos anteriormente (función `my_df`). \n",
    "\n",
    "- Declaramos que el formato en que vamos a cargar el archivo en MinIO va a ser Parquet con `loader_file_format=\"parquet\"`.\n",
    "\n",
    "- En caso de que la tabla ya exista, declaramos `write_disposition=\"replace\"` para que se sobrescriba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9fc2b2-b417-4aa8-ae1d-e4ef138365cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_info = pipeline.run(\n",
    "    my_df,\n",
    "    loader_file_format=\"parquet\",\n",
    "    write_disposition=\"replace\"\n",
    ")\n",
    "print(load_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
